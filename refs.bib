@article {ZKRT18,
	author = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
	title = {Efficient compression in color naming and its evolution},
	year = {2018},
	doi = {10.1073/pnas.1800521115},
    journal={Proceedings of the National Academy of Sciences},
	publisher = {National Academy of Sciences}
    }
    
    @article{KYR18,
author = {Kemp, Charles and Xu, Yang and Regier, Terry},
title = {Semantic Typology and Efficient Communication},
journal = {Annual Review of Linguistics},
volume = {4},
number = {1},
pages = {109-128},
year = {2018}
}

@book{CT06,
  author = {Cover, Thomas M. and Thomas, Joy A.},
  publisher = {Wiley-Interscience},
  title = {Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing)},
  year = 2006
}

@article{Thompson1933,
 author = {William R. Thompson},
 journal = {Biometrika},
 number = {3/4},
 pages = {285--294},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples},
 volume = {25},
 year = {1933}
}

@InProceedings{Gal2015, title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning}, author = {Yarin Gal and Zoubin Ghahramani}, booktitle = {Proceedings of The 33rd International Conference on Machine Learning}, pages = {1050--1059}, year = {2016}, volume = {48}, series = {Proceedings of Machine Learning Research}, month = {20--22 Jun},, abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.} }
@article{Srivastava2014,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
}

@misc{Kingma2014,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@misc{Riquelme2018,
    title={Deep Bayesian Bandits Showdown: An Empirical Comparison of Bayesian Deep Networks for Thompson Sampling},
    author={Carlos Riquelme and George Tucker and Jasper Snoek},
    year={2018},
    eprint={1802.09127},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@misc{Collier2018,
    title={Deep Contextual Multi-armed Bandits},
    author={Mark Collier and Hector Urdiales Llorens},
    year={2018},
    eprint={1807.09809},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}
% From Mendeley
@article{Regier2007,
abstract = {The nature of color categories in the world's languages is contested. One major view holds that color categories are organized around universal focal colors, whereas an opposing view holds instead that categories are defined at their boundaries by linguistic convention. Both of these standardly opposed views are challenged by existing data. Here, we argue for a third view based on a proposal by Jameson and D'Andrade [Jameson KA, D'Andrade RG (1997) in Color Categories in Thought and Language, eds Hardin CL, Maff i L (Cambridge Univ Press, Cambridge, U.K.), pp 295-319]: that color naming across languages reflects optimal or near-optimal divisions of an irregularly shaped perceptual color space. We formalize this idea, test it against color-naming data from a broad range of languages and show that it accounts for universal tendencies in color naming while also accommodating some observed cross-language variation. {\textcopyright} 2007 by The National Academy of Sciences of the USA.},
author = {Regier, Terry and Kay, Paul and Khetarpal, Naveen},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Cognitive modeling,Color categories,Color terms,Semantic universels},
number = {4},
pages = {1436--1441},
title = {{Color naming reflects optimal partitions of color space}},
volume = {104},
year = {2007}
}
@article{Havrylov2017,
abstract = {},
archivePrefix = {arXiv},
arxivId = {1705.11192},
author = {Havrylov, Serhii and Titov, Ivan},
eprint = {1705.11192},
file = {:Users/caremil/Library/Application Support/Mendeley Desktop/Downloaded/Havrylov, Titov - 2017 - Emergence of language with multi-agent games Learning to communicate with sequences of symbols.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {2150--2160},
title = {{Emergence of language with multi-agent games: Learning to communicate with sequences of symbols}},
volume = {2017-Decem},
year = {2017}
}

@article{Kageback2020,
abstract = {},
author = {K{\aa}geb{\"{a}}ck, Mikael and Carlsson, Emil and Dubhashi, Devdatt and Sayeed, Asad},
journal = {PLoS ONE},
number = {7},
pages = {1--26},
pmid = {32667959},
title = {{A reinforcement-learning approach to efficient communication}},
volume = {15},
year = {2020}
}

@article{Kemp2012,
author = {Kemp, Charles and Regier, Terry},
year = {2012},
month = {05},
pages = {1049-54},
title = {Kinship Categories Across Languages Reflect General Communicative Principles},
volume = {336},
journal = {Science (New York, N.Y.)},
}

@article{Regier2015,
abstract = {Why do languages have the semantic categories they do? Each language partitions human experience into a system of semantic categories, labeled by words or morphemes, which are used to communicate about experience. These categories often differ widely across ... \n},
author = {Regier, Terry and Kemp, Charles and Kay, Paul},
journal = {The Handbook of Language Emergence},
number = {January 2015},
pages = {237--263},
title = {{Word Meanings across Languages Support Efficient Communication}},
year = {2015}
}

@article{Xu2020,
abstract = {},
author = {Xu, Yang and Liu, Emmy and Regier, Terry},
journal = {Open Mind},
keywords = {efficient communication,functionalism,number,recursion,semantic typology},
pages = {57--70},
title = {{Numeral Systems Across Languages Support Efficient Communication: From Approximate Numerosity to Recursion}},
volume = {4},
year = {2020}
}
@article{Pica2004,
abstract = {},
author = {Pica, Pierre and Lemer, Cathy and Izard, V{\'{e}}ronique and Dehaene, Stanislas},
journal = {Science},
number = {5695},
pages = {499--503},
title = {{Exact and approximate arithmetic in an Amazonian indigene group}},
volume = {306},
year = {2004}
}

@article{Zaslavsky2018a,
abstract = {We derive a principled information-theoretic account of cross-language semantic variation. Specifically, we argue that languages efficiently compress ideas into words by optimizing the information bottleneck (IB) trade-off between the complexity and accuracy of the lexicon. We test this proposal in the domain of color naming and show that (i) color-naming systems across languages achieve near-optimal compression; (ii) small changes in a single trade-off parameter account to a large extent for observed cross-language variation; (iii) efficient IB color-naming systems exhibit soft rather than hard category boundaries and often leave large regions of color space inconsistently named, both of which phenomena are found empirically; and (iv) these IB systems evolve through a sequence of structural phase transitions, in a single process that captures key ideas associated with different accounts of color category evolution. These results suggest that a drive for information-theoretic efficiency may shape color-naming systems across languages. This principle is not specific to color, and so it may also apply to cross-language variation in other semantic domains.},
author = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {Categories,Color naming,Information theory,Language evolution,Semantic typology},
number = {31},
pages = {7937--7942},
title = {{Efficient compression in color naming and its evolution}},
volume = {115},
year = {2018}
}

@article{Jorge2016,
abstract = {},
archivePrefix = {arXiv},
arxivId = {1611.03218},
author = {Jorge, Emilio and K{\aa}geb{\"{a}}ck, Mikael and Johansson, Fredrik D. and Gustavsson, Emil},
eprint = {1611.03218},
file = {:Users/caremil/Library/Application Support/Mendeley Desktop/Downloaded/Jorge et al. - 2016 - Learning to Play Guess Who and Inventing a Grounded Language as a Consequence.pdf:pdf},
title = {{Learning to Play Guess Who? and Inventing a Grounded Language as a Consequence}},
year = {2016}
}

@inproceedings{Lazaridou2017,
abstract = {},
archivePrefix = {arXiv},
arxivId = {1612.07182},
author = {Lazaridou, Angeliki and Peysakhovich, Alexander and Baroni, Marco},
booktitle = {5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings},
eprint = {1612.07182},
pages = {1--11},
title = {{Multi-agent cooperation and the emergence of (natural) language}},
year = {2017}
}

@article {Gibson2016,
	author = {Gibson, Edward and Futrell, Richard and Jara-Ettinger, Julian and Mahowald, Kyle and Bergen, Leon and Ratnasingam, Sivalogeswaran and Gibson, Mitchell and Piantadosi, Steven T. and Conway, Bevil R.},
	title = {Color naming across languages reflects color use},
	elocation-id = {201619666},
	year = {2017},
	abstract = {},
	issn = {0027-8424},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{Zaslavsky2019a,
	Author = {Zaslavsky, Noga and Kemp, Charles and Tishby, Naftali and Regier, Terry},
	Title = {Color Naming Reflects Both Perceptual Structure and Communicative Need},
	Journal = {Topics in Cognitive Science},
	Year = {2019},
	Volume = {11},
	Number = {1},
	Pages = {207--219},
}

@article {Michel2011,
	author = {Michel, Jean-Baptiste and Shen, Yuan Kui and Aiden, Aviva Presser and Veres, Adrian and Gray, Matthew K. and , and Pickett, Joseph P. and Hoiberg, Dale and Clancy, Dan and Norvig, Peter and Orwant, Jon and Pinker, Steven and Nowak, Martin A. and Aiden, Erez Lieberman},
	title = {Quantitative Analysis of Culture Using Millions of Digitized Books},
	volume = {331},
	number = {6014},
	pages = {176--182},
	year = {2011},,
	abstract = {We constructed a corpus of digitized texts containing about 4\% of all books ever printed. Analysis of this corpus enables us to investigate cultural trends quantitatively. We survey the vast terrain of {\textquoteleft}culturomics,{\textquoteright} focusing on linguistic and cultural phenomena that were reflected in the English language between 1800 and 2000. We show how this approach can provide insights about fields as diverse as lexicography, the evolution of grammar, collective memory, the adoption of technology, the pursuit of fame, censorship, and historical epidemiology. Culturomics extends the boundaries of rigorous quantitative inquiry to a wide array of new phenomena spanning the social sciences and the humanities.},
	issn = {0036-8075},
	journal = {Science}
}

@article{Kemp2018,
author = {Kemp, Charles and Xu, Yang and Regier, Terry},
year = {2018},
month = {01},
pages = {109-128},
title = {Semantic Typology and Efficient Communication},
volume = {4},
journal = {Annual Review of Linguistics},
}

@article{Gibson2019,
title = "How Efficiency Shapes Human Language",
journal = "Trends in Cognitive Sciences",
volume = "23",
number = "5",
pages = "389 - 407",
year = "2019",
issn = "1364-6613",
author = "Edward Gibson and Richard Futrell and Steven P. Piantadosi and Isabelle Dautriche and Kyle Mahowald and Leon Bergen and Roger Levy",
keywords = "language evolution, communication, language efficiency, cross-linguistic universals, language learnability, language complexity",
abstract = "Cognitive science applies diverse tools and perspectives to study human language. Recently, an exciting body of work has examined linguistic phenomena through the lens of efficiency in usage: what otherwise puzzling features of language find explanation in formal accounts of how language might be optimized for communication and learning? Here, we review studies that deploy formal tools from probability and information theory to understand how and why language works the way that it does, focusing on phenomena ranging from the lexicon through syntax. These studies show how a pervasive pressure for efficiency guides the forms of natural language and indicate that a rich future for language research lies in connecting linguistics to cognitive psychology and mathematical theories of communication and inference."
}

@book{Lewis1969,
	author = {David K. Lewis},
	title = {Convention: A Philosophical Study},
	year = {1969},
	publisher = {Wiley-Blackwell}
}

@article{Schulz2019,
title = "The algorithmic architecture of exploration in the human brain",
journal = "Current Opinion in Neurobiology",
volume = "55",
pages = "7 - 14",
year = "2019",
note = "Machine Learning, Big Data, and Neuroscience",
author = "Eric Schulz and Samuel J. Gershman",
abstract = "Balancing exploration and exploitation is one of the central problems in reinforcement learning. We review recent studies that have identified multiple algorithmic strategies underlying exploration. In particular, humans use a combination of random and uncertainty-directed exploration strategies, which rely on different brain systems, have different developmental trajectories, and are sensitive to different task manipulations. Humans are also able to exploit sophisticated structural knowledge to aid their exploration, such as information about correlations between options. New computational models, drawing inspiration from machine learning, have begun to formalize these ideas and offer new ways to understand the neural basis of reinforcement learning."
}

@article{Botvinick2019,
author = {Botvinick, Mathew and Ritter, Sam and Wang, Jane and Kurth-Nelson, Zeb and Blundell, Charles and Hassabis, Demis},
year = {2019},
month = {04},
pages = {},
title = {Reinforcement Learning, Fast and Slow},
volume = {23},
journal = {Trends in Cognitive Sciences},
}

@article{Dabney2020,
author = {Dabney, Will and Kurth-Nelson, Zeb and Uchida, Naoshige and Starkweather, Clara and Hassabis, Demis and Munos, Remi and Botvinick, Matthew},
year = {2020},
month = {01},
pages = {1-5},
title = {A distributional code for value in dopamine-based reinforcement learning},
volume = {577},
journal = {Nature},
}

@article{Wilson2014,
author = {Wilson, Robert and Geana, Andra and White, John and Ludvig, Elliot and Cohen, Jonathan},
year = {2014},
month = {10},
pages = {},
title = {Humans Use Directed and Random Exploration to Solve the Explore-Exploit Dilemma.},
volume = {143},
journal = {Journal of experimental psychology. General},
}

@book{Fechner1966,
author= {Gustav Fechner},
year = {1966},
title = {Elements of psychophysics},
volume = {1},
publisher={Holt, Rinehart and Winston: New York}
}

@book{Marr82,
author= {D. Marr},
year = {1982},
title = {Vision: A Computational Approach},
publisher={San Francisco, Freeman & Co.}
}

@article{Poggio12,
  title={The Levels of Understanding framework, revised},
  author={Tomaso Poggio},
  journal={Perception},
  year={2012},
  volume={41},
  pages={1017 - 1023}
}

@article {Eckstein20,
	author = {Eckstein, Maria K. and Collins, Anne G. E.},
	title = {Computational evidence for hierarchically structured reinforcement learning in humans},
	volume = {117},
	number = {47},
	pages = {29381--29389},
	year = {2020},
	publisher = {National Academy of Sciences},
	journal = {Proceedings of the National Academy of Sciences}
}


@article{Piazza2013,
  title={Education Enhances the Acuity of the Nonverbal Approximate Number System},
  author={M. Piazza and P. Pica and V. Izard and E. Spelke and S. Dehaene},
  journal={Psychological Science},
  year={2013},
  volume={24},
  pages={1037 - 1043}
}

@incollection{wals-131,
  address   = {Leipzig},
  author    = {Bernard Comrie},
  booktitle = {The World Atlas of Language Structures Online},
  editor    = {Matthew S. Dryer and Martin Haspelmath},
  publisher = {Max Planck Institute for Evolutionary Anthropology},
  title     = {Numeral Bases},
  year      = {2013}
}

@book{Sutton1998,
  author = {Sutton, Richard S. and Barto, Andrew G.},
  edition = {Second},
  publisher = {The MIT Press},
  title = {Reinforcement Learning: An Introduction}, 
  year = {1998}
  }
  
 @article{Niv2005,
author = {Niv, Yael and Duff, Michael and Dayan, Peter},
year = {2005},
month = {06},
pages = {6},
title = {Dopamine, uncertainty and TD learning},
volume = {1},
journal = {Behavioral and brain functions : BBF},
}

@inproceedings{Zaslavsky2019c,
	Author = {Zaslavsky, Noga and Regier, Terry and Tishby, Naftali and Kemp, Charles},
	Title = {Semantic categories of artifacts and animals reflect efficient coding},
	Booktitle = {41st Annual Conference of the {C}ognitive {S}cience {S}ociety},
	Year = {2019},
}

@article{Conway2019,
author = {Conway, Bevil and Ratnasingam, Sivalogeswaran and Jara-Ettinger, Julian and Futrell, Richard and Gibson, Edward},
year = {2019},
month = {11},
pages = {104086},
title = {Communication efficiency of color naming across languages provides a new framework for the evolution of color terms},
volume = {195},
journal = {Cognition},
}

@article{Sun2012,
title = "A framework for Bayesian optimality of psychophysical laws",
journal = "Journal of Mathematical Psychology",
volume = "56",
number = "6",
pages = "495 - 501",
year = "2012",
author = "John Z. Sun and Grace I. Wang and Vivek K Goyal and Lav R. Varshney",
keywords = "Relative Error, Bayesian Quantization, Weber–Fechner, Psychophysical Scale",
abstract = "The Weber–Fechner law states that perceived intensity is proportional to physical stimuli on a logarithmic scale. In this work, we formulate a Bayesian framework for the scaling of perception and find logarithmic and related scalings are optimal under expected relative error fidelity. Therefore, the Weber–Fechner law arises as being information theoretically efficient under the constraint of limited representability. An even stronger connection is drawn between the Weber–Fechner law and a Bayesian framework when neural storage or communication is the dominant concern, such as for numerosity. Theoretical results and experimental verification for perception of sound intensity are both presented."
}

@article{Harald2010,
author = {Hammarström, H.},
year = {2010},
month = {01},
pages = {},
title = {Rarities in Numeral Systems},
journal = {Business Communication Quarterly - Bus Comm Q}
}

@article{Thiel2019,
title={A Non–Convex Optimization Approach to Correlation Clustering},
volume={33},
number={01}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Thiel, Erik and Chehreghani, Morteza Haghir and Dubhashi, Devdatt},
year={2019},
month={Jul.},
pages={5159-5166} 
}


@article{Bansal2004,
	abstract = {We consider the following clustering problem: we have a complete graph on n vertices (items), where each edge (u, v) is labeled either + or −depending on whether u and v have been deemed to be similar or different. The goal is to produce a partition of the vertices (a clustering) that agrees as much as possible with the edge labels. That is, we want a clustering that maximizes the number of + edges within clusters, plus the number of −edges between clusters (equivalently, minimizes the number of disagreements: the number of −edges inside clusters plus the number of + edges between clusters). This formulation is motivated from a document clustering problem in which one has a pairwise similarity function f learned from past data, and the goal is to partition the current set of documents in a way that correlates with f as much as possible; it can also be viewed as a kind of ``agnostic learning''problem.},
	author = {Bansal, Nikhil and Blum, Avrim and Chawla, Shuchi},
	id = {Bansal2004},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {89--113},
	title = {Correlation Clustering},
	ty = {JOUR},
	volume = {56},
	year = {2004}}
	
	@article{Auer2002,
	Author = {Auer, Peter and Cesa-Bianchi, Nicol{\`o} and Fischer, Paul},
	Id = {Auer2002},
	Isbn = {1573-0565},
	Journal = {Machine Learning},
	Number = {2},
	Pages = {235--256},
	Title = {Finite-time Analysis of the Multiarmed Bandit Problem},
	Ty = {JOUR},
	Volume = {47},
	Year = {2002}
	}
	@misc{graves2014neural,
      title={Neural Turing Machines}, 
      author={Alex Graves and Greg Wayne and Ivo Danihelka},
      year={2014},
      eprint={1410.5401},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}


@article{DNC,
	Abstract = {Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read--write memory.},
	Author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'n}ska, Agnieszka and Colmenarejo, Sergio G{\'o}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`a}Puigdom{\`e}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
	Da = {2016/10/01},
	Date-Added = {2020-11-15 21:28:17 +0100},
	Date-Modified = {2020-11-15 21:28:17 +0100},
	Doi = {10.1038/nature20101},
	Id = {Graves2016},
	Isbn = {1476-4687},
	Journal = {Nature},
	Number = {7626},
	Pages = {471--476},
	Title = {Hybrid computing using a neural network with dynamic external memory},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/nature20101},
	Volume = {538},
	Year = {2016},
	Bdsk-Url-1 = {https://doi.org/10.1038/nature20101}}

